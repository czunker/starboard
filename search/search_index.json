{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Starboard Starboard integrates security tools into the Kubernetes environment, so that users can find and view the risks that relate to different resources in a Kubernetes-native way. Starboard provides custom security resources definitions and a Go module to work with a range of existing security tools, as well as a kubectl -compatible executable command, and an Octant plug-in that make security reports available through familiar Kubernetes tools. Starboard can be run in two different modes: As a command-line tool, so you can trigger scans and view the risks in a kubectl-compatible way or as part of your CI/CD pipeline. As an operator to automatically update security report resources in response to workload and other changes on a Kubernetes cluster - for example, initiating a vulnerability scan when a new pod is started. You can read more about the motivations and use cases here and join our discussions here.","title":"Introduction"},{"location":"#welcome-to-starboard","text":"Starboard integrates security tools into the Kubernetes environment, so that users can find and view the risks that relate to different resources in a Kubernetes-native way. Starboard provides custom security resources definitions and a Go module to work with a range of existing security tools, as well as a kubectl -compatible executable command, and an Octant plug-in that make security reports available through familiar Kubernetes tools. Starboard can be run in two different modes: As a command-line tool, so you can trigger scans and view the risks in a kubectl-compatible way or as part of your CI/CD pipeline. As an operator to automatically update security report resources in response to workload and other changes on a Kubernetes cluster - for example, initiating a vulnerability scan when a new pod is started. You can read more about the motivations and use cases here and join our discussions here.","title":"Welcome to Starboard"},{"location":"configuration/","text":"Configuration The starboard init command creates the starboard ConfigMap and the starboard secret in the starboard namespace with default configuration settings. Similarly, the operator ensures the starboard ConfigMap and the starboard secret in the OPERATOR_NAMESPACE . You can change the default settings with kubectl patch or kubectl edit commands. For example, by default Trivy displays vulnerabilities with all severity levels ( UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL ). However, you can opt in to display only HIGH and CRITICAL vulnerabilities by patching the trivy.severity value in the starboard ConfigMap: kubectl patch cm starboard -n <starboard_operator> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.severity\": \"HIGH,CRITICAL\" } } EOF )\" To set the GitHub token used by Trivy in Standalone mode add the trivy.githubToken value to the starboard secret instead: GITHUB_TOKEN=<your token> kubectl patch secret starboard -n <starboard_operator> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.githubToken\": \"$(echo -n $GITHUB_TOKEN | base64)\" } } EOF )\" The following tables list available configuration parameters with their default values. NOTE You only need to configure the settings for the scanner you are using (i.e. trivy.* parameters are used if vulnerabilityReports.scanner is set to Trivy ). Check integrations page to see example configuration settings for common use cases. CONFIGMAP KEY DEFAULT DESCRIPTION vulnerabilityReports.scanner Trivy The name of the scanner that generates vulnerability reports. Either Trivy or Aqua . trivy.httpProxy N/A The HTTP proxy used by Trivy to download the vulnerabilities database from GitHub. Only applicable in Standalone mode. trivy.severity UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL A comma separated list of severity levels reported by Trivy trivy.imageRef docker.io/aquasec/trivy:0.14.0 Trivy image reference trivy.mode Standalone Trivy client mode. Either Standalone or ClientServer . Depending on the active mode other settings might be applicable or required. trivy.serverURL N/A The endpoint URL of the Trivy server. Required in ClientServer mode. trivy.serverTokenHeader Trivy-Token The name of the HTTP header to send the authentication token to Trivy server. Only application in ClientServer mode when trivy.serverToken is specified. aqua.imageRef docker.io/aquasec/scanner:5.3 Aqua scanner image reference. The tag determines the version of the scanner binary executable and it must be compatible with version of Aqua console. aqua.serverURL N/A The endpoint URL of Aqua management console polaris.config.yaml Check the default value here Polaris configuration file SECRET KEY DESCRIPTION trivy.githubToken The GitHub access token used by Trivy to download the vulnerabilities database from GitHub. Only applicable in Standalone mode. trivy.serverToken The token to authenticate Trivy client with Trivy server. Only applicable in ClientServer mode. trivy.serverCustomHeaders A comma-separated list of custom HTTP headers sent by Trivy client to Trivy server. Only applicable in ClientServer mode. aqua.username Aqua management console username aqua.password Aqua management console password NOTE You can find it handy to delete a configuration key, which was not created by default by the starboard init command. For example, the following kubectl patch command deletes the trivy.httpProxy key: kubectl patch cm starboard -n <starboard_operator> \\ --type json \\ -p '[{\"op\": \"remove\", \"path\": \"/data/trivy.httpProxy\"}]'","title":"Configuration"},{"location":"configuration/#configuration","text":"The starboard init command creates the starboard ConfigMap and the starboard secret in the starboard namespace with default configuration settings. Similarly, the operator ensures the starboard ConfigMap and the starboard secret in the OPERATOR_NAMESPACE . You can change the default settings with kubectl patch or kubectl edit commands. For example, by default Trivy displays vulnerabilities with all severity levels ( UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL ). However, you can opt in to display only HIGH and CRITICAL vulnerabilities by patching the trivy.severity value in the starboard ConfigMap: kubectl patch cm starboard -n <starboard_operator> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.severity\": \"HIGH,CRITICAL\" } } EOF )\" To set the GitHub token used by Trivy in Standalone mode add the trivy.githubToken value to the starboard secret instead: GITHUB_TOKEN=<your token> kubectl patch secret starboard -n <starboard_operator> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.githubToken\": \"$(echo -n $GITHUB_TOKEN | base64)\" } } EOF )\" The following tables list available configuration parameters with their default values. NOTE You only need to configure the settings for the scanner you are using (i.e. trivy.* parameters are used if vulnerabilityReports.scanner is set to Trivy ). Check integrations page to see example configuration settings for common use cases. CONFIGMAP KEY DEFAULT DESCRIPTION vulnerabilityReports.scanner Trivy The name of the scanner that generates vulnerability reports. Either Trivy or Aqua . trivy.httpProxy N/A The HTTP proxy used by Trivy to download the vulnerabilities database from GitHub. Only applicable in Standalone mode. trivy.severity UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL A comma separated list of severity levels reported by Trivy trivy.imageRef docker.io/aquasec/trivy:0.14.0 Trivy image reference trivy.mode Standalone Trivy client mode. Either Standalone or ClientServer . Depending on the active mode other settings might be applicable or required. trivy.serverURL N/A The endpoint URL of the Trivy server. Required in ClientServer mode. trivy.serverTokenHeader Trivy-Token The name of the HTTP header to send the authentication token to Trivy server. Only application in ClientServer mode when trivy.serverToken is specified. aqua.imageRef docker.io/aquasec/scanner:5.3 Aqua scanner image reference. The tag determines the version of the scanner binary executable and it must be compatible with version of Aqua console. aqua.serverURL N/A The endpoint URL of Aqua management console polaris.config.yaml Check the default value here Polaris configuration file SECRET KEY DESCRIPTION trivy.githubToken The GitHub access token used by Trivy to download the vulnerabilities database from GitHub. Only applicable in Standalone mode. trivy.serverToken The token to authenticate Trivy client with Trivy server. Only applicable in ClientServer mode. trivy.serverCustomHeaders A comma-separated list of custom HTTP headers sent by Trivy client to Trivy server. Only applicable in ClientServer mode. aqua.username Aqua management console username aqua.password Aqua management console password NOTE You can find it handy to delete a configuration key, which was not created by default by the starboard init command. For example, the following kubectl patch command deletes the trivy.httpProxy key: kubectl patch cm starboard -n <starboard_operator> \\ --type json \\ -p '[{\"op\": \"remove\", \"path\": \"/data/trivy.httpProxy\"}]'","title":"Configuration"},{"location":"crds/","text":"Custom Resource Definitions This project houses CustomResourceDefinitions (CRDs) related to security and compliance checks along with the code generated by Kubernetes code generators to write such custom resources in a natural way. NAME SHORTNAMES APIGROUP NAMESPACED KIND vulnerabilityreports vulns,vuln aquasecurity.github.io true VulnerabilityReport configauditreports configaudit aquasecurity.github.io true ConfigAuditReport ciskubebenchreports kubebench aquasecurity.github.io false CISKubeBenchReport kubehunterreports kubehunter aquasecurity.github.io false KubeHunterReport","title":"Custom Resource Definitions"},{"location":"crds/#custom-resource-definitions","text":"This project houses CustomResourceDefinitions (CRDs) related to security and compliance checks along with the code generated by Kubernetes code generators to write such custom resources in a natural way. NAME SHORTNAMES APIGROUP NAMESPACED KIND vulnerabilityreports vulns,vuln aquasecurity.github.io true VulnerabilityReport configauditreports configaudit aquasecurity.github.io true ConfigAuditReport ciskubebenchreports kubebench aquasecurity.github.io false CISKubeBenchReport kubehunterreports kubehunter aquasecurity.github.io false KubeHunterReport","title":"Custom Resource Definitions"},{"location":"faq/","text":"Frequently Asked Questions Why do you duplicate instances of VulnerabilityReports for the same image digest? Docker image reference is not a first class citizen in Kubernetes. It's a property of the container definition. Starboard relies on label selectors to associate VulnerabilityReports with corresponding Kubernetes workloads, not particular image references. For example, we can get all reports for the wordpress Deployment with the following command: kubectl get vulnerabilityreports \\ -l starboard.resource.kind=Deployment \\ -l starboard.resource.name=wordpress Beyond that, for each instance of the VulnerabilityReports we set the owner reference pointing to the corresponding pods controller. By doing that we can manage orphaned VulnerabilityReports and leverage Kubernetes garbage collection. For example, if the wordpress Deployment is deleted, all related VulnerabilityReports are automatically garbage collected. Why do you create an instance of the VulnerabilityReport for each container? The idea is to partition VulnerabilityReports generated for a particular Kubernetes workload by containers is to mitigate the risk of exceeding the etcd request payload limit. By default, the payload of each Kubernetes object stored etcd is subject to 1.5 MiB. Is Starboard CLI required to run Starboard Operator or vice versa? No. Starboard CLI and Starboard Operator are independent applications, even though they use compatible interfaces to create or read security reports. For example, a VulnerabilityReports created by the Starboard Operator can be retrieved with the Starboard CLI's get command.","title":"Frequently Asked Questions"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/#why-do-you-duplicate-instances-of-vulnerabilityreports-for-the-same-image-digest","text":"Docker image reference is not a first class citizen in Kubernetes. It's a property of the container definition. Starboard relies on label selectors to associate VulnerabilityReports with corresponding Kubernetes workloads, not particular image references. For example, we can get all reports for the wordpress Deployment with the following command: kubectl get vulnerabilityreports \\ -l starboard.resource.kind=Deployment \\ -l starboard.resource.name=wordpress Beyond that, for each instance of the VulnerabilityReports we set the owner reference pointing to the corresponding pods controller. By doing that we can manage orphaned VulnerabilityReports and leverage Kubernetes garbage collection. For example, if the wordpress Deployment is deleted, all related VulnerabilityReports are automatically garbage collected.","title":"Why do you duplicate instances of VulnerabilityReports for the same image digest?"},{"location":"faq/#why-do-you-create-an-instance-of-the-vulnerabilityreport-for-each-container","text":"The idea is to partition VulnerabilityReports generated for a particular Kubernetes workload by containers is to mitigate the risk of exceeding the etcd request payload limit. By default, the payload of each Kubernetes object stored etcd is subject to 1.5 MiB.","title":"Why do you create an instance of the VulnerabilityReport for each container?"},{"location":"faq/#is-starboard-cli-required-to-run-starboard-operator-or-vice-versa","text":"No. Starboard CLI and Starboard Operator are independent applications, even though they use compatible interfaces to create or read security reports. For example, a VulnerabilityReports created by the Starboard Operator can be retrieved with the Starboard CLI's get command.","title":"Is Starboard CLI required to run Starboard Operator or vice versa?"},{"location":"integrations/","text":"Integrations Vulnerability Scanners Trivy Standalone The default configuration settings enable Trivy vulnerabilityReports.scanner in Standalone trivy.mode . Even though it doesn't require any additional setup, it's the least efficient method. Each pod created by a scan job has the init container that downloads the Trivy vulnerabilities database from the GitHub releases page and stores it in the local file system of an emptyDir volume. This volume is then shared with the main containers that perform the actual scanning. Finally, the pod is deleted along with the emptyDir volume. The number of containers defined by a scan job equals the number of containers defined by the scanned Kubernetes workload, so the cache in this mode is useful only if the workload defines multiple containers. Beyond that, frequent downloads from GitHub might lead to a rate limiting problem. The limits are imposed by GitHub on all anonymous requests originating from a given IP. To mitigate such problems you can add the trivy.githubToken key to the starboard secret. GITHUB_TOKEN=<your token> kubectl patch secret starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.githubToken\": \"$(echo -n $GITHUB_TOKEN | base64)\" } } EOF )\" Trivy ClientServer You can connect Starboard to an external Trivy server by changing the default trivy.mode from Standalone to ClientServer and specifying trivy.serverURL . TRIVY_SERVER_URL=<your server URL> kubectl patch cm starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.mode\": \"ClientServer\", \"trivy.serverURL\": \"$TRIVY_SERVER_URL\" } } EOF )\" The Trivy server could be your own deployment, or it could be an external service. See Trivy documentation for more information on deploying Trivy in ClientServer mode. If the server requires access token and / or custom HTTP authentication headers, you may add trivy.serverToken and trivy.serverCustomHeaders properties to the starboard secret. SERVER_TOKEN=<your server token> X_API_TOKEN=<your API token> kubectl patch secret starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.serverToken\": \"$(echo -n $SERVER_TOKEN | base64)\", \"trivy.serverCustomHeaders\": \"$(echo -n x-api-token:$X_API_TOKEN | base64)\" } } EOF )\" Aqua You can use Aqua's commercial scanner to scan container images and generate vulnerability reports. The Starboard connector for Aqua attempts to fetch the vulnerability report for the specified image digest via Aqua's API. If the report is not found, it spins up an ad-hoc scan by executing the scannercli command. The value of aqua.imageRef determines the version of the actual scannercli binary executable and must be compatible with the version of your Aqua deployment. By default, scannercli 5.3 is used, but if you are running, for example, Aqua 5.2, change the value to docker.io/aquasec/scanner:5.2 . To integrate Aqua scanner change the value of the vulnerabilityReports.scanner property to Aqua and specify the aqua.serverURL : AQUA_SERVER_URL=<your console URL> kubectl patch cm starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"vulnerabilityReports.scanner\": \"Aqua\", \"aqua.serverURL\": \"$AQUA_SERVER_URL\" } } EOF )\" Finally, edit the starboard secret to configure aqua.username and aqua.password credentials, which are used to connect to the Aqua's management console: AQUA_CONSOLE_USERNAME=<your username> AQUA_CONSOLE_PASSWORD=<your password> kubectl patch secret starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"aqua.username\": \"$(echo -n $AQUA_CONSOLE_USERNAME | base64)\", \"aqua.password\": \"$(echo -n $AQUA_CONSOLE_PASSWORD | base64)\" } } EOF )\" Private Registries Image Pull Secrets Find references to image pull secrets (direct references and via service account). Create the temporary secret with basic credentials for each container of the scanned workload. Create the scan job that references the temporary secret. The secret has the ownerReference property set to point to the job. Watch the job until it's completed or failed. Parse logs and save vulnerability reports in etcd. Delete the job. The temporary secret will be deleted by the Kubernetes garbage collector. Managed Registries Amazon Elastic Container Registry (ECR) You must create an IAM OIDC identity provider for your cluster: eksctl utils associate-iam-oidc-provider \\ --cluster <cluster_name> \\ --approve Assuming that the operator is installed in the <starboard_operator_namespace> namespace you can override the existing starboard-operator service account and attach the IAM policy to grant it permission to pull images from the ECR: eksctl create iamserviceaccount \\ --name starboard-operator \\ --namespace <starboard_operator_namespace> \\ --cluster <cluster_name> \\ --attach-policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly \\ --approve \\ --override-existing-serviceaccounts","title":"Integrations"},{"location":"integrations/#integrations","text":"","title":"Integrations"},{"location":"integrations/#vulnerability-scanners","text":"","title":"Vulnerability Scanners"},{"location":"integrations/#trivy-standalone","text":"The default configuration settings enable Trivy vulnerabilityReports.scanner in Standalone trivy.mode . Even though it doesn't require any additional setup, it's the least efficient method. Each pod created by a scan job has the init container that downloads the Trivy vulnerabilities database from the GitHub releases page and stores it in the local file system of an emptyDir volume. This volume is then shared with the main containers that perform the actual scanning. Finally, the pod is deleted along with the emptyDir volume. The number of containers defined by a scan job equals the number of containers defined by the scanned Kubernetes workload, so the cache in this mode is useful only if the workload defines multiple containers. Beyond that, frequent downloads from GitHub might lead to a rate limiting problem. The limits are imposed by GitHub on all anonymous requests originating from a given IP. To mitigate such problems you can add the trivy.githubToken key to the starboard secret. GITHUB_TOKEN=<your token> kubectl patch secret starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.githubToken\": \"$(echo -n $GITHUB_TOKEN | base64)\" } } EOF )\"","title":"Trivy Standalone"},{"location":"integrations/#trivy-clientserver","text":"You can connect Starboard to an external Trivy server by changing the default trivy.mode from Standalone to ClientServer and specifying trivy.serverURL . TRIVY_SERVER_URL=<your server URL> kubectl patch cm starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.mode\": \"ClientServer\", \"trivy.serverURL\": \"$TRIVY_SERVER_URL\" } } EOF )\" The Trivy server could be your own deployment, or it could be an external service. See Trivy documentation for more information on deploying Trivy in ClientServer mode. If the server requires access token and / or custom HTTP authentication headers, you may add trivy.serverToken and trivy.serverCustomHeaders properties to the starboard secret. SERVER_TOKEN=<your server token> X_API_TOKEN=<your API token> kubectl patch secret starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"trivy.serverToken\": \"$(echo -n $SERVER_TOKEN | base64)\", \"trivy.serverCustomHeaders\": \"$(echo -n x-api-token:$X_API_TOKEN | base64)\" } } EOF )\"","title":"Trivy ClientServer"},{"location":"integrations/#aqua","text":"You can use Aqua's commercial scanner to scan container images and generate vulnerability reports. The Starboard connector for Aqua attempts to fetch the vulnerability report for the specified image digest via Aqua's API. If the report is not found, it spins up an ad-hoc scan by executing the scannercli command. The value of aqua.imageRef determines the version of the actual scannercli binary executable and must be compatible with the version of your Aqua deployment. By default, scannercli 5.3 is used, but if you are running, for example, Aqua 5.2, change the value to docker.io/aquasec/scanner:5.2 . To integrate Aqua scanner change the value of the vulnerabilityReports.scanner property to Aqua and specify the aqua.serverURL : AQUA_SERVER_URL=<your console URL> kubectl patch cm starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"vulnerabilityReports.scanner\": \"Aqua\", \"aqua.serverURL\": \"$AQUA_SERVER_URL\" } } EOF )\" Finally, edit the starboard secret to configure aqua.username and aqua.password credentials, which are used to connect to the Aqua's management console: AQUA_CONSOLE_USERNAME=<your username> AQUA_CONSOLE_PASSWORD=<your password> kubectl patch secret starboard -n <starboard_namespace> \\ --type merge \\ -p \"$(cat <<EOF { \"data\": { \"aqua.username\": \"$(echo -n $AQUA_CONSOLE_USERNAME | base64)\", \"aqua.password\": \"$(echo -n $AQUA_CONSOLE_PASSWORD | base64)\" } } EOF )\"","title":"Aqua"},{"location":"integrations/#private-registries","text":"","title":"Private Registries"},{"location":"integrations/#image-pull-secrets","text":"Find references to image pull secrets (direct references and via service account). Create the temporary secret with basic credentials for each container of the scanned workload. Create the scan job that references the temporary secret. The secret has the ownerReference property set to point to the job. Watch the job until it's completed or failed. Parse logs and save vulnerability reports in etcd. Delete the job. The temporary secret will be deleted by the Kubernetes garbage collector.","title":"Image Pull Secrets"},{"location":"integrations/#managed-registries","text":"","title":"Managed Registries"},{"location":"integrations/#amazon-elastic-container-registry-ecr","text":"You must create an IAM OIDC identity provider for your cluster: eksctl utils associate-iam-oidc-provider \\ --cluster <cluster_name> \\ --approve Assuming that the operator is installed in the <starboard_operator_namespace> namespace you can override the existing starboard-operator service account and attach the IAM policy to grant it permission to pull images from the ECR: eksctl create iamserviceaccount \\ --name starboard-operator \\ --namespace <starboard_operator_namespace> \\ --cluster <cluster_name> \\ --attach-policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly \\ --approve \\ --override-existing-serviceaccounts","title":"Amazon Elastic Container Registry (ECR)"},{"location":"operator/","text":"Starboard Operator Overview This operator automatically updates security report resources in response to workload and other changes on a Kubernetes cluster - for example, initiating a vulnerability scan when a new pod is started. In other words, the desired state for this operator is that for each workload there are security reports stored in the cluster as custom resources. Currently, the operator only supports vulnerabilityreports security resources as depicted below. However, we plan to support all custom security resources. Installation kubectl You can install the operator with provided static YAML manifests with fixed values. However, this approach has its shortcomings. For example, if you want to change the container image or modify default configuration parameters, you have to create new manifests or edit existing ones. To deploy the operator in the starboard-operator namespace and configure it to watch the default namespace: Send the definition of the vulnerabilityreports custom resource to the Kubernetes API: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/crd/vulnerabilityreports.crd.yaml Send the following Kubernetes objects definitions to the Kubernetes API: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/01-starboard-operator.ns.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/02-starboard-operator.sa.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/03-starboard-operator.clusterrole.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/04-starboard-operator.clusterrolebinding.yaml (Optional) Configure the operator by creating the starboard ConfigMap and the starboard secret in the starboard-operator namespace. If you skip this step, the operator will ensure configuration objects on startup with the default settings. kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/05-starboard-operator.config.yaml Review the default values and makes sure the operator is configured properly: kubectl describe cm starboard -n starboard-operator kubectl describe secret starboard -n starboard-operator Finally, create the starboard-operator Deployment in the starboard-operator namespace to start the operator's pod: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/06-starboard-operator.deployment.yaml To confirm that the operator is running, check the number of replicas created by the starboard-operator Deployment in the starboard-operator namespace: kubectl get deployment -n starboard-operator You should see the output similar to the following: NAME READY UP-TO-DATE AVAILABLE AGE starboard-operator 1/1 1 1 11m If for some reason it's not ready yet, check the logs of the Deployment for errors: kubectl logs -n starboard-operator deployment/starboard-operator In case of any error consult our Troubleshooting guidelines. You can uninstall the operator with the following command: kubectl delete -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/06-starboard-operator.deployment.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/05-starboard-operator.cm.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/04-starboard-operator.clusterrolebinding.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/03-starboard-operator.clusterrole.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/02-starboard-operator.sa.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/01-starboard-operator.ns.yaml Helm Helm , which is de facto standard package manager for Kubernetes, allows installing applications from parameterized YAML manifests called Helm charts . To address shortcomings of static YAML manifests we provide the Helm chart to deploy the Starboard operator. The Helm chart supports all install modes . As an example, let's install the operator in the starboard-operator namespace and configure it to watch the default namespaces: Clone the chart repository: git clone https://github.com/aquasecurity/starboard.git cd starboard (Optional) Configure the operator by creating the starboard ConfigMap and the starboard secret in the starboard-operator namespace. If you skip this step, the operator will ensure configuration objects on startup with the default settings. kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/05-starboard-operator.config.yaml Review the default values and makes sure the operator is configured properly: kubectl describe cm starboard -n starboard-operator kubectl describe secret starboard -n starboard-operator Install the chart: helm install starboard-operator ./deploy/helm \\ -n starboard-operator \\ --create-namespace \\ --set=\"targetNamespaces=default\" Check that the starboard-operator Helm release is created in the starboard-operator namespace: helm list -n starboard-operator NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION starboard-operator starboard-operator 1 2020-12-09 16:15:51.070673 +0100 CET deployed starboard-operator-0.2.1 0.7.1 To confirm that the operator is running, check the number of replicas created by the starboard-operator Deployment in the starboard-operator namespace: kubectl get deployment -n starboard-operator You should see the output similar to the following: NAME READY UP-TO-DATE AVAILABLE AGE starboard-operator 1/1 1 1 11m If for some reason it's not ready yet, check the logs of the Deployment for errors: kubectl logs -n starboard-operator deployment/starboard-operator In case of any error consult our Troubleshooting guidelines. You can uninstall the operator with the following command: helm uninstall starboard-operator -n starboard-operator NOTE You have to manually delete CRDs created by the helm install command: kubectl delete crd vulnerabilityreports.aquasecurity.github.io Getting Started Assuming that you installed the operator in the starboard-operator namespace, and it's configured to discover Kubernetes workloads in the default namespace, let's create the nginx Deployment that we know is vulnerable: kubectl create deployment nginx --image nginx:1.16 When the first pod controlled by the nginx Deployment is created, the operator immediately detects that and creates the Kubernetes job in the starboard-operator namespace to scan the nignx container's image ( nginx:1.16 ) for vulnerabilities. kubectl get job -n starboard-operator In our case you should see only one job with a random name scheduled to scan the nginx Deployment: NAME COMPLETIONS DURATION AGE 69516243-c782-4445-88b4-689ffbb3cdb7 0/1 68s 68s If everything goes fine, the scan job is deleted, and the operator creates the vulnerabilityreports resource in the default namespace that corresponds to the nginx container: kubectl get vulnerabilityreports -o wide Notice that the vulnerabilityreports instance is associated with the active ReplicaSet of the nginx Deployment by name, and the set of labels. NAME REPOSITORY TAG SCANNER AGE CRITICAL HIGH MEDIUM LOW UNKNOWN replicaset-nginx-6d4cf56db6-nginx library/nginx 1.16 Trivy 14m 1 35 16 85 2 You can get and describe vulnerabilityreports as any other built-in Kubernetes object. For example, you can display the report as JSON object: kubectl get vulnerabilityreports replicaset-nginx-6d4cf56db6-nginx -o json Configuration Configuration of the operator's pod is done via environment variables at startup. NAME DEFAULT DESCRIPTION OPERATOR_NAMESPACE N/A See Install modes OPERATOR_TARGET_NAMESPACES N/A See Install modes OPERATOR_LOG_DEV_MODE false The flag to use (or not use) development mode (more human-readable output, extra stack traces and logging information, etc). OPERATOR_SCAN_JOB_TIMEOUT 5m The length of time to wait before giving up on a scan job OPERATOR_METRICS_BIND_ADDRESS :8080 The TCP address to bind to for serving Prometheus metrics. It can be set to 0 to disable the metrics serving. OPERATOR_HEALTH_PROBE_BIND_ADDRESS :9090 The TCP address to bind to for serving health probes, i.e. /healthz/ and /readyz/ endpoints. Install Modes The values of the OPERATOR_NAMESPACE and OPERATOR_TARGET_NAMESPACES determine the install mode, which in turn determines the multitenancy support of the operator. MODE OPERATOR_NAMESPACE OPERATOR_TARGET_NAMESPACES DESCRIPTION OwnNamespace operators operators The operator can be configured to watch events in the namespace it is deployed in. SingleNamespace operators foo The operator can be configured to watch for events in a single namespace that the operator is not deployed in. MultiNamespace operators foo,bar,baz The operator can be configured to watch for events in more than one namespace. AllNamespaces operators (blank string) The operator can be configured to watch for events in all namespaces.","title":"Starboard Operator"},{"location":"operator/#starboard-operator","text":"","title":"Starboard Operator"},{"location":"operator/#overview","text":"This operator automatically updates security report resources in response to workload and other changes on a Kubernetes cluster - for example, initiating a vulnerability scan when a new pod is started. In other words, the desired state for this operator is that for each workload there are security reports stored in the cluster as custom resources. Currently, the operator only supports vulnerabilityreports security resources as depicted below. However, we plan to support all custom security resources.","title":"Overview"},{"location":"operator/#installation","text":"","title":"Installation"},{"location":"operator/#kubectl","text":"You can install the operator with provided static YAML manifests with fixed values. However, this approach has its shortcomings. For example, if you want to change the container image or modify default configuration parameters, you have to create new manifests or edit existing ones. To deploy the operator in the starboard-operator namespace and configure it to watch the default namespace: Send the definition of the vulnerabilityreports custom resource to the Kubernetes API: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/crd/vulnerabilityreports.crd.yaml Send the following Kubernetes objects definitions to the Kubernetes API: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/01-starboard-operator.ns.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/02-starboard-operator.sa.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/03-starboard-operator.clusterrole.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/04-starboard-operator.clusterrolebinding.yaml (Optional) Configure the operator by creating the starboard ConfigMap and the starboard secret in the starboard-operator namespace. If you skip this step, the operator will ensure configuration objects on startup with the default settings. kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/05-starboard-operator.config.yaml Review the default values and makes sure the operator is configured properly: kubectl describe cm starboard -n starboard-operator kubectl describe secret starboard -n starboard-operator Finally, create the starboard-operator Deployment in the starboard-operator namespace to start the operator's pod: kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/06-starboard-operator.deployment.yaml To confirm that the operator is running, check the number of replicas created by the starboard-operator Deployment in the starboard-operator namespace: kubectl get deployment -n starboard-operator You should see the output similar to the following: NAME READY UP-TO-DATE AVAILABLE AGE starboard-operator 1/1 1 1 11m If for some reason it's not ready yet, check the logs of the Deployment for errors: kubectl logs -n starboard-operator deployment/starboard-operator In case of any error consult our Troubleshooting guidelines. You can uninstall the operator with the following command: kubectl delete -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/06-starboard-operator.deployment.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/05-starboard-operator.cm.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/04-starboard-operator.clusterrolebinding.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/03-starboard-operator.clusterrole.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/02-starboard-operator.sa.yaml \\ -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/01-starboard-operator.ns.yaml","title":"kubectl"},{"location":"operator/#helm","text":"Helm , which is de facto standard package manager for Kubernetes, allows installing applications from parameterized YAML manifests called Helm charts . To address shortcomings of static YAML manifests we provide the Helm chart to deploy the Starboard operator. The Helm chart supports all install modes . As an example, let's install the operator in the starboard-operator namespace and configure it to watch the default namespaces: Clone the chart repository: git clone https://github.com/aquasecurity/starboard.git cd starboard (Optional) Configure the operator by creating the starboard ConfigMap and the starboard secret in the starboard-operator namespace. If you skip this step, the operator will ensure configuration objects on startup with the default settings. kubectl apply -f https://raw.githubusercontent.com/aquasecurity/starboard/main/deploy/static/05-starboard-operator.config.yaml Review the default values and makes sure the operator is configured properly: kubectl describe cm starboard -n starboard-operator kubectl describe secret starboard -n starboard-operator Install the chart: helm install starboard-operator ./deploy/helm \\ -n starboard-operator \\ --create-namespace \\ --set=\"targetNamespaces=default\" Check that the starboard-operator Helm release is created in the starboard-operator namespace: helm list -n starboard-operator NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION starboard-operator starboard-operator 1 2020-12-09 16:15:51.070673 +0100 CET deployed starboard-operator-0.2.1 0.7.1 To confirm that the operator is running, check the number of replicas created by the starboard-operator Deployment in the starboard-operator namespace: kubectl get deployment -n starboard-operator You should see the output similar to the following: NAME READY UP-TO-DATE AVAILABLE AGE starboard-operator 1/1 1 1 11m If for some reason it's not ready yet, check the logs of the Deployment for errors: kubectl logs -n starboard-operator deployment/starboard-operator In case of any error consult our Troubleshooting guidelines. You can uninstall the operator with the following command: helm uninstall starboard-operator -n starboard-operator NOTE You have to manually delete CRDs created by the helm install command: kubectl delete crd vulnerabilityreports.aquasecurity.github.io","title":"Helm"},{"location":"operator/#getting-started","text":"Assuming that you installed the operator in the starboard-operator namespace, and it's configured to discover Kubernetes workloads in the default namespace, let's create the nginx Deployment that we know is vulnerable: kubectl create deployment nginx --image nginx:1.16 When the first pod controlled by the nginx Deployment is created, the operator immediately detects that and creates the Kubernetes job in the starboard-operator namespace to scan the nignx container's image ( nginx:1.16 ) for vulnerabilities. kubectl get job -n starboard-operator In our case you should see only one job with a random name scheduled to scan the nginx Deployment: NAME COMPLETIONS DURATION AGE 69516243-c782-4445-88b4-689ffbb3cdb7 0/1 68s 68s If everything goes fine, the scan job is deleted, and the operator creates the vulnerabilityreports resource in the default namespace that corresponds to the nginx container: kubectl get vulnerabilityreports -o wide Notice that the vulnerabilityreports instance is associated with the active ReplicaSet of the nginx Deployment by name, and the set of labels. NAME REPOSITORY TAG SCANNER AGE CRITICAL HIGH MEDIUM LOW UNKNOWN replicaset-nginx-6d4cf56db6-nginx library/nginx 1.16 Trivy 14m 1 35 16 85 2 You can get and describe vulnerabilityreports as any other built-in Kubernetes object. For example, you can display the report as JSON object: kubectl get vulnerabilityreports replicaset-nginx-6d4cf56db6-nginx -o json","title":"Getting Started"},{"location":"operator/#configuration","text":"Configuration of the operator's pod is done via environment variables at startup. NAME DEFAULT DESCRIPTION OPERATOR_NAMESPACE N/A See Install modes OPERATOR_TARGET_NAMESPACES N/A See Install modes OPERATOR_LOG_DEV_MODE false The flag to use (or not use) development mode (more human-readable output, extra stack traces and logging information, etc). OPERATOR_SCAN_JOB_TIMEOUT 5m The length of time to wait before giving up on a scan job OPERATOR_METRICS_BIND_ADDRESS :8080 The TCP address to bind to for serving Prometheus metrics. It can be set to 0 to disable the metrics serving. OPERATOR_HEALTH_PROBE_BIND_ADDRESS :9090 The TCP address to bind to for serving health probes, i.e. /healthz/ and /readyz/ endpoints.","title":"Configuration"},{"location":"operator/#install-modes","text":"The values of the OPERATOR_NAMESPACE and OPERATOR_TARGET_NAMESPACES determine the install mode, which in turn determines the multitenancy support of the operator. MODE OPERATOR_NAMESPACE OPERATOR_TARGET_NAMESPACES DESCRIPTION OwnNamespace operators operators The operator can be configured to watch events in the namespace it is deployed in. SingleNamespace operators foo The operator can be configured to watch for events in a single namespace that the operator is not deployed in. MultiNamespace operators foo,bar,baz The operator can be configured to watch for events in more than one namespace. AllNamespaces operators (blank string) The operator can be configured to watch for events in all namespaces.","title":"Install Modes"},{"location":"troubleshooting/","text":"Troubleshooting \"starboard\" cannot be opened because the developer cannot be verified. (macOS) Since Starboard CLI is not registered with Apple by an identified developer, if you try to run it for the first time you might get a warning dialog. This doesn't mean that something is wrong with the release binary, rather macOS can't check whether the binary has been modified or broken since it was released. To override your security settings and use the Starboard CLI anyway, follow these steps: In the Finder on your Mac, locate the starboard binary. Control-click the binary icon, then choose Open from the shortcut menu. Click Open. The starboard is saved as an exception to your security settings, and you can use it just as you can any registered app. You can also grant an exception for a blocked Starboard release binary by clicking the Allow Anyway button in the General pane of Security & Privacy preferences. This button is available for about an hour after you try to run the Starboard CLI command. To open this pane on your Mac, choose Apple menu > System Preferences , click Security & Privacy , then click General .","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"troubleshooting/#starboard-cannot-be-opened-because-the-developer-cannot-be-verified-macos","text":"Since Starboard CLI is not registered with Apple by an identified developer, if you try to run it for the first time you might get a warning dialog. This doesn't mean that something is wrong with the release binary, rather macOS can't check whether the binary has been modified or broken since it was released. To override your security settings and use the Starboard CLI anyway, follow these steps: In the Finder on your Mac, locate the starboard binary. Control-click the binary icon, then choose Open from the shortcut menu. Click Open. The starboard is saved as an exception to your security settings, and you can use it just as you can any registered app. You can also grant an exception for a blocked Starboard release binary by clicking the Allow Anyway button in the General pane of Security & Privacy preferences. This button is available for about an hour after you try to run the Starboard CLI command. To open this pane on your Mac, choose Apple menu > System Preferences , click Security & Privacy , then click General .","title":"\"starboard\" cannot be opened because the developer cannot be verified. (macOS)"}]}